{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1C2mOs1c05yCm8L9U8TNW0HH4dbBkvEW5","timestamp":1694757730971}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UKc8K9_57UEz"},"source":["# CS171-EE142 - Fall 2022 - Homework 4\n","\n","# Due: Friday, November 3, 2022 @ 11:59pm\n","\n","### Maximum points: 50 pts\n","\n","\n","## Submit your solution to Gradescope:\n","1. Submit a single PDF to **HW4**\n","2. Submit your jupyter notebook to **HW4-code**\n","\n","Notice:\n","In Markdown, when you write in LaTeX math mode, do not leave any leading and trailing whitespaces inside the dollar signs ($). For example, write `(dollarSign)\\mathbf(dollarSign)(dollarSign)` instead of `(dollarSign)(space)\\mathbf{w}(dollarSign)`. Otherwise, nbconvert will throw an error and the generated pdf will be incomplete. [This is a bug of nbconvert.](https://tex.stackexchange.com/questions/367176/jupyter-notebook-latex-conversion-fails-escaped-and-other-symbols)\n","\n","**See the additional submission instructions at the end of this notebook**"]},{"cell_type":"markdown","metadata":{"id":"JL5tIX4c9z6s"},"source":["\n","## Enter your information below:\n","\n","### Your Name (submitter):\n","\n","### Your student ID (submitter):\n","    \n","    \n","<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n","\n","\n","## Academic Integrity\n","Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.\n"]},{"cell_type":"code","metadata":{"id":"fJQhDvSz3BK9"},"source":["# Standard library imports.\n","import random as rand\n","\n","# Related third party imports.\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","# Local application/library specific imports.\n","# import here if you write .py script"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_7FVGGI_aqre"},"source":["### Question 1. Logistic regression [50 pts]\n","\n","In this question, we will plot the logistic function and perform logistic regression. We will use the breast cancer data set.  This data set is described here:\n","\n","https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin\n","\n","Each sample is a collection of features that were manually recorded by a physician upon inspecting a sample of cells from fine needle aspiration.  The goal is to detect if the cells are benign or malignant.  \n","\n","We could use the `sklearn` built-in `LogisticRegression` class to find the weights for the logistic regression problem.  The `fit` routine in that class has an *optimizer* to select the weights to best match the data.  To understand how that optimizer works, in this problem, we will build a very simple gradient descent optimizer from scratch.  "]},{"cell_type":"markdown","metadata":{"id":"EHaz8CZBfUVr"},"source":["### Loading and visualizing the Breast Cancer Data\n","\n","We load the data from the UCI site and remove the missing values."]},{"cell_type":"code","metadata":{"id":"4RdmLqolffmw"},"source":["names = ['id','thick','size_unif','shape_unif','marg','cell_size','bare',\n","         'chrom','normal','mit','class']\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/' +\n","                 'breast-cancer-wisconsin/breast-cancer-wisconsin.data',\n","                names=names,na_values='?',header=None)\n","df = df.dropna()\n","df.head(6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EEPMsFmfpqS"},"source":["After loading the data, we can create a scatter plot of the data labeling the class values with different colors.  We will pick two of the features.  "]},{"cell_type":"code","metadata":{"id":"rXSf9mKtfwbF"},"source":["# Get the response.  Convert to a zero-one indicator\n","yraw = np.array(df['class'])\n","BEN_VAL = 2   # value in the 'class' label for benign samples\n","MAL_VAL = 4   # value in the 'class' label for malignant samples\n","y = (yraw == MAL_VAL).astype(int)\n","Iben = (y==0)\n","Imal = (y==1)\n","\n","# Get two predictors\n","xnames =['size_unif','marg']\n","X = np.array(df[xnames])\n","\n","# Create the scatter plot\n","plt.plot(X[Imal,0],X[Imal,1],'r.')\n","plt.plot(X[Iben,0],X[Iben,1],'g.')\n","plt.xlabel(xnames[0], fontsize=16)\n","plt.ylabel(xnames[1], fontsize=16)\n","plt.ylim(0,14)\n","plt.legend(['malign','benign'],loc='upper right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UY5fLe90gCnU"},"source":["The above plot is not informative, since many of the points are on top of one another.  Thus, we cannot see the relative frequency of points.  "]},{"cell_type":"markdown","metadata":{"id":"1RaZ9ToBgD6-"},"source":["### Logistic function\n","\n","We will build a binary classifier using *logistic regression*.  In logistic regression, we do not just output an estimate of the class label.  Instead, we ouput a *probability*, an estimate of how likely the sample is one class or the other.  That is our output is a number from 0 to 1 representing the likelihood:\n","$$\n","    P(y = 1|x)\n","$$\n","which is our estimate of the probability that the sample is one class (in this case, a malignant sample) based on the features in `x`.  This is sometimes called a *soft classifier*.  \n","\n","In logistic regression, we assume that likelihood is of the form\n","$$\n","    P(y=1|x) = \\sigma(z),  \\quad z = w(1)x(1) + \\cdots + w(d)x(d) + b = \\mathbf{w}^T\\mathbf{x}+b,  \n","$$\n","where $w(1),\\ldots,w(d),b$ are the classifier weights and $\\sigma(z)$ is the so-called *logistic* function:\n","$$\n","    \\sigma(z) = \\frac{1}{1+e^{-z}}.\n","$$\n","\n","To understand the logistic function, suppose $x$ is a scalar and samples $y$ are drawn with $P(y=1|x) = f(w x+b)$ for some $w$ and $b$.  We plot these samples for different $w,b$."]},{"cell_type":"code","metadata":{"id":"shwwuK9Ogv7D"},"source":["N = 100\n","xm = 20\n","ws = np.array([0.5,1,2,10])\n","bs = np.array([0, 5, -5])\n","wplot = ws.size\n","bplot = bs.size\n","iplot = 0\n","for b in bs:\n","  for w in ws:\n","    iplot += 1\n","    x  = np.random.uniform(-xm,xm,N)\n","\n","    py = 1/(1+np.exp(-w*x-b))\n","\n","    yp = np.array(np.random.rand(N) < py) # hard label for random points\n","    xp = np.linspace(-xm,xm,100)\n","    pyp = 1/(1+np.exp(-w*xp-b)) # soft label (probability) for the points\n","\n","    plt.subplot(bplot,wplot,iplot)\n","\n","    plt.scatter(x,yp,c=yp,edgecolors='none',marker='+')\n","    plt.plot(xp,pyp,'b-')\n","    plt.axis([-xm,xm,-0.1,1.1])\n","    plt.grid()\n","    if ((iplot%4)!=1):\n","        plt.yticks([])\n","    plt.xticks([-20,-10,0,10,20])\n","    plt.title('w={0:.1f}, b={1:.1f}'.format(w,b))\n","\n","    plt.subplots_adjust(top=1.5, bottom=0.2, hspace=0.5, wspace=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kyhRv_1hhOj3"},"source":["We see that $\\sigma(wx+b)$ represents the probability that $y=1$.  The function $\\sigma(wx) > 0.5$ for $x>0$ meaning the samples are more likely to be $y=1$.  Similarly, for $x<0$, the samples are more likely to be $y=0$.  The scaling $w$ determines how fast that transition is and $b$ influences the transition point.  "]},{"cell_type":"markdown","metadata":{"id":"Dr2_5WR2cY9p"},"source":["### Question 1a: Fitting the Logistic Model on Two Variables using Library Implementation [10 pts]\n","\n","We will fit the logistic model on the two variables `size_unif` and `marg` that we were looking at earlier.\n","\n","Follow the instructions on the cell to complete the code."]},{"cell_type":"code","metadata":{"id":"WQPUSMlTcY9q"},"source":["# Create a list named xnames that contains 2 elements: 'size_unif' and 'marg'\n","# xnames = ...\n","\n","# Take the two columns from the data stored in df variable and convert it to numpy array. Store the result in variable X.\n","# X = ...\n","\n","# Print the shape of variable X\n","# print(...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zvgB0iCCcY9q"},"source":["Next we split the data into training and test"]},{"cell_type":"code","metadata":{"id":"2oC7pomCcY9q"},"source":["# Split into training and test\n","from sklearn.model_selection import train_test_split\n","\n","# Split the data into train and test set using the train_test_split function setting test_size=0.30\n","# Xtr, Xts, ytr, yts = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qr4ru1WYcY9v"},"source":["**Logistic regression in scikit-learn**\n","\n","The actual fitting is easy with the `sklearn` package.  The parameter `C`\n","states the level of inverse regularization strength with higher values meaning less regularization. Right now, we will select a high value to minimally regularize the estimate.\n","\n","We can also measure the accuracy on the test data. You should get an accuracy around 90%.\n","\n","Follow the instructions on the cell to complete the code."]},{"cell_type":"code","metadata":{"id":"lKGhsUYfzUx3"},"source":["from sklearn import datasets, linear_model, preprocessing\n","\n","# Create the Logistic Regression model using the library class linear_model.LogisticRegression setting C to 1e5\n","# reg = ...\n","\n","# Fit the training data to the model calling the fit() funtion\n","# ...\n","\n","# Print the coefficients and intercept of the decision function learnt. They are stored in coef_ and intercept_ attributes of the model\n","# print(...)\n","# print(...)\n","\n","# Generate the prediction on the test set by calling the predict() function\n","# yhat = ...\n","\n","# Calculate the mean accuracy and print it like \"Accuracy on test data = 0.xxxxxx\"\n","# acc = ...\n","# print(\"Accuracy on test data = %f\" % acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOhsTYG6zXDk"},"source":["**Now, we will implement the regression function using gradient descent.**"]},{"cell_type":"markdown","metadata":{"id":"KCTbi1wTcbIn"},"source":["### Question 1b. Gradient descent for logistic regression [20 pts]\n","In the class we saw that the weight vector can be found by minimizing the negative log likelihood over $N$ training samples.  The negative log likelihood is called the *loss* function.  For the logistic regression problem, the loss function simplifies to\n","\n","$$L(\\mathbf{w}) = - \\sum_{i=1}^N y_i \\log \\sigma(\\mathbf{w}^T\\mathbf{x}_i+b) + (1-y_i)\\log [1-\\sigma(\\mathbf{w}^T\\mathbf{x}_i+b)].$$\n","\n","Gradient can be computed as $$\\nabla_\\mathbf{w}L = \\sum_{i=1}^N(\\sigma(\\mathbf{w}^T\\mathbf{x}_i)-y_i)\\mathbf{x}_i ,~~~ \\nabla_b L = \\sum_{i=1}^N(\\sigma(\\mathbf{w}^T\\mathbf{x}_i)-y_i).$$\n","\n","\n","We can update $\\mathbf{w},b$ at every iteration as  \n","$$ \\mathbf{w} \\gets \\mathbf{w} - \\alpha \\nabla_\\mathbf{w}L, \\\\ b \\gets b - \\alpha \\nabla_b L.$$\n","\n","**Note that we could also append the constant term in $\\mathbf{w}$ and append 1 to every $\\mathbf{x}_i$ accordingly, but we kept them separate in the expressions above.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QNa10nUiS5ru"},"source":["**Gradient descent function implementation**\n","\n","We will use this loss function and gradient to implement a gradient descent-based method for logistic regression.\n","\n","Recall that training a logistic function means finding a weight vector `w` for the classification rule:\n","\n","    P(y=1|x,w) = 1/(1+\\exp(-z)), z = w[0]+w[1]*x[1] + ... + w[d]x[d]\n","    \n","The function implemented should follow the following format:\n","```python\n","def logistic_regression_gd(X,y,learning_rate = 0.001,max_iter=1000,tol=pow(10,-5)):\n","```\n","Where `X` is the training data feature(s), `y` is the variable to be predicted, `learning_rate` is the learning rate used ($\\alpha$ in the slides), `max_iter` defines the maximum number of iterations that gradient descent is allowed to run, and `tol` is defining the tolerance for convergence (which we'll discuss next).\n","\n","The return values for the above function should be (at the least) 1) `w` which are the regression parameters, 2) `all_cost` which is an array where each position contains the value of the objective function $L(\\mathbf{w})$ for a given iteration, 3) `iters` which counts how many iterations did the algorithm need in order to converge to a solution.\n","\n","Gradient descent is an iterative algorithm; it keeps updating the variables until a convergence criterion is met. In our case, our convergence criterion is whichever of the following two criteria happens first:\n","\n","- The maximum number of iterations is met\n","- The relative improvement in the cost is not greater than the tolerance we have specified. For this criterion, you may use the following snippet into your code:\n","```python\n","np.absolute(all_cost[it] - all_cost[it-1])/all_cost[it-1] <= tol\n","```"]},{"cell_type":"code","metadata":{"id":"SRDxzsZk8ck_"},"source":["# TODO\n","# Your code for logistic regression via gradient descent goes here\n","\n","def compute_cost(X,w,y):\n","    # your code for the loss function goes here\n","\n","    return L\n","\n","def logistic_regression_gd(X,y,learning_rate = 0.00001,max_iter=1000,tol=pow(10,-5)):\n","    # your code goes here\n","\n","    return w, all_cost, iters"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"08r6_M-98clK"},"source":["### Question 1c: Convergence plots and test accuracy [10 pts]\n","\n","After implementing gradient descent for logistic regression, we would like to test that indeed our algorithm converges to a solution. In order see this, we are going to look at the value of the objective/loss function $L(\\mathbf{w})$ as a function of the number of iterations, and ideally, what we would like to see is $L(\\mathbf{w})$ drops as we run more iterations, and eventually it stabilizes.\n","\n","The learning rate plays a big role in how fast our algorithm converges: a larger learning rate means that the algorithm is making faster strides to the solution, whereas a smaller learning rate implies slower steps. In this question we are going to test two different values for the learning rate:\n","- 0.001\n","- 0.00001\n","\n","while keeping the default values for the max number of iterations and the tolerance.\n","\n","\n","- Plot the two convergence plots (cost vs. iterations)\n","- Calculate the accuracy of classifier on the test data `Xts`\n","- What do you observe?\n"]},{"cell_type":"markdown","metadata":{"id":"2XWJPz_uU1zT"},"source":["**Calculate accuracy of your classifier on test data**\n","\n","To calculate the accuracy of our classifier on the test data, we can create a predict method.\n","\n","Implement a function `predict(X,w)` that provides you label 1 if $\\mathbf{w}^T\\mathbf{x} + b > 0$ and 0 otherwise.  "]},{"cell_type":"code","metadata":{"id":"uFyHcnVFVD4r"},"source":["# TODO\n","# Predict on test samples and measure accuracy\n","def predict(X,w):\n","  # your code goes here\n","\n","  return yhat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7KAVMGV8clL"},"source":["# TODO\n","# test gradient descent with step size 0.001\n","# test gradient descent with step size 0.00001\n","\n","(w, all_cost,iters) = logistic_regression_gd(Xtr,ytr,learning_rate = 0.001,max_iter = 1000, tol=pow(10,-6))\n","plt.semilogy(all_cost[0:iters])\n","plt.grid()\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')\n","\n","yhat = predict(Xts,w)\n","acc = np.mean(yhat == yts)\n","print(\"Test accuracy = %f\" % acc)\n","\n","# complete the rest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zu5HpI_-8clL"},"source":["Observations:\n","\n","1.\n","2.\n","\n"]},{"cell_type":"markdown","source":["### Question 1d: Print the misclassified data from the test set [10 pts]\n","\n","As the accuracy is less than 100%, some data are misclassified. Print the misclassified data in the following format:\n","\n","`\n","Input data: [a b] Original label: 1 Predicted label: 0\n","`"],"metadata":{"id":"309Hf_L5u33E"}},{"cell_type":"code","source":["# Your code for printing the misclassified data\n"],"metadata":{"id":"LkI6W1Biu6nm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Which class is showing more misclassifications?\n","* **Answer:**\n"],"metadata":{"id":"zxXa8mw7u_Tn"}},{"cell_type":"markdown","source":["---\n","## Submission instructions\n","1. Download this Colab to ipynb, and convert it to PDF. Follow similar steps as [here](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab) but convert to PDF.\n"," - Download your .ipynb file. You can do it using only Google Colab. `File` -> `Download` -> `Download .ipynb`\n"," - Reupload it so Colab can see it. Click on the `Files` icon on the far left to expand the side bar. You can directly drag the downloaded .ipynb file to the area. Or click `Upload to session storage` icon and then select & upload your .ipynb file.\n"," - Conversion using %%shell.\n"," ```\n","!sudo apt-get update\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n","!jupyter nbconvert --log-level CRITICAL --to pdf name_of_hw.ipynb\n","  ```\n"," - Your PDF file is ready. Click 3 dots and `Download`.\n","\n","\n","  \n","\n","2. Upload the PDF to Gradescope, select the correct pdf pages for each question. **Important!**\n","\n","3. Upload the ipynb file to Gradescope"],"metadata":{"id":"WecX82dMRrVo"}},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"],"metadata":{"id":"ub1l1I4ZRqSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --log-level CRITICAL --to pdf \"/content/fall2023_hw4.ipynb\" # make sure the file path is correct and delete this comment before running this cell"],"metadata":{"id":"M2XuF-qZ_-fk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kMCJzii39h4d"},"execution_count":null,"outputs":[]}]}